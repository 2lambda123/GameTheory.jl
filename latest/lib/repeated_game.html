<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Repeated Game · Games.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js" data-main="../assets/documenter.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/></head><body><nav class="toc"><h1>Games.jl</h1><select id="version-selector" onChange="window.location.href=this.value" style="visibility: hidden"></select><form class="search" action="../search.html"><input id="search-query" name="q" type="text" placeholder="Search docs"/></form><ul><li><a class="toctext" href="../index.html">Home</a></li><li><span class="toctext">Library</span><ul><li><a class="toctext" href="normal_form_game.html">Normal Form Game</a></li><li><a class="toctext" href="computing_nash_equilibria.html">Computing Nash Equilibria</a></li><li class="current"><a class="toctext" href="repeated_game.html">Repeated Game</a><ul class="internal"><li><a class="toctext" href="#repeated_game_util-1">Repeated Game Utilities</a></li><li><a class="toctext" href="#repeated_game-1">Repeated Game</a></li></ul></li><li><a class="toctext" href="random.html">Random</a></li><li><a class="toctext" href="index.html">Index</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Library</li><li><a href="repeated_game.html">Repeated Game</a></li></ul><a class="edit-page" href="https://github.com/QuantEcon/Games.jl/tree/cafd8e9206d4126611afce9ffcbdc472b5e06bcc/docs/src/lib/repeated_game.md"><span class="fa"></span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Repeated Game</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" id="Repeated-Game-1" href="#Repeated-Game-1">Repeated Game</a></h1><p>This is documentation for Repeated Game.</p><h2><a class="nav-anchor" id="repeated_game_util-1" href="#repeated_game_util-1">Repeated Game Utilities</a></h2><p>Documentation for <code>repeated_game_util.jl</code>.</p><h3><a class="nav-anchor" id="Exported-1" href="#Exported-1">Exported</a></h3><h3><a class="nav-anchor" id="Internal-1" href="#Internal-1">Internal</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Games.unitcircle-Tuple{Int64}" href="#Games.unitcircle-Tuple{Int64}"><code>Games.unitcircle</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Places <code>npts</code> equally spaced points along the 2 dimensional circle and returns the points with x coordinates in first column and y coordinates in second column</p><p>i.e. if you wanted point i, it would be pts[i, :]</p></div><a class="source-link" target="_blank" href="https://github.com/QuantEcon/Games.jl/tree/cafd8e9206d4126611afce9ffcbdc472b5e06bcc/src/repeated_game_util.jl#L6-L13">source</a><br/></section><h2><a class="nav-anchor" id="repeated_game-1" href="#repeated_game-1">Repeated Game</a></h2><p>Documentation for <code>repeated_game.jl</code>.</p><h3><a class="nav-anchor" id="Exported-2" href="#Exported-2">Exported</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Games.RepeatedGame" href="#Games.RepeatedGame"><code>Games.RepeatedGame</code></a> — <span class="docstring-category">Type</span>.</div><div><p>This is a type for a specific type of repeated games</p><p>It takes a stage game that is repeated in every period and all agents discount future at rate δ</p></div><a class="source-link" target="_blank" href="https://github.com/QuantEcon/Games.jl/tree/cafd8e9206d4126611afce9ffcbdc472b5e06bcc/src/repeated_game.jl#L8-L13">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Games.RepeatedGame-Tuple{Games.Player,Games.Player,Float64}" href="#Games.RepeatedGame-Tuple{Games.Player,Games.Player,Float64}"><code>Games.RepeatedGame</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Helper constructor that builds game from players</p></div><a class="source-link" target="_blank" href="https://github.com/QuantEcon/Games.jl/tree/cafd8e9206d4126611afce9ffcbdc472b5e06bcc/src/repeated_game.jl#L26-L28">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Games.outerapproximation-Tuple{Games.RepeatedGame{2,T} where T&lt;:Real}" href="#Games.outerapproximation-Tuple{Games.RepeatedGame{2,T} where T&lt;:Real}"><code>Games.outerapproximation</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Approximates the set of equilibrium value set for a repeated game with the outer hyperplane approximation described by Judd, Yeltekin, Conklin 2002</p><p>NOTE: If your code fails then it might be the case that the value set is       only the value which corresponds to the pure action nash equilibrium       or you might just need more points to be precise enough for the       Polyhedra library to be able to convert to the vertice representation.</p><p>The arguments are</p><ul><li><p>rpd: 2 player repeated game</p></li></ul><p>The keyword arguments are</p><ul><li><p>nH: Number of subgradients used in approximation</p></li><li><p>tol: Tolerance in differences of set</p></li><li><p>maxiter: Maximum number of iterations</p></li><li><p>verbose: Whether to display updates about iterations and distance</p></li><li><p>nskipprint: Number of iterations between printing information (verbose=true)</p></li><li><p>check_pure_nash: Whether to perform a check about whether a pure Nash equilibrium exists</p></li><li><p>plib: Allows users to choose a particular package for the geometry computations (See <a href="https://github.com/JuliaPolyhedra/Polyhedra.jl">Polyhedra.jl</a>       docs for more info). By default, it chooses to use <a href="https://github.com/JuliaPolyhedra/CDDLib.jl">CDDLib.jl</a></p></li></ul></div><a class="source-link" target="_blank" href="https://github.com/QuantEcon/Games.jl/tree/cafd8e9206d4126611afce9ffcbdc472b5e06bcc/src/repeated_game.jl#L209-L231">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Games.unpack-Tuple{Games.RepeatedGame}" href="#Games.unpack-Tuple{Games.RepeatedGame}"><code>Games.unpack</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Unpacks the elements of a repeated game</p></div><a class="source-link" target="_blank" href="https://github.com/QuantEcon/Games.jl/tree/cafd8e9206d4126611afce9ffcbdc472b5e06bcc/src/repeated_game.jl#L32-L34">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Games.worst_value_1-Tuple{Games.RepeatedGame{2,T} where T&lt;:Real,Array{Float64,2},Array{Float64,1}}" href="#Games.worst_value_1-Tuple{Games.RepeatedGame{2,T} where T&lt;:Real,Array{Float64,2},Array{Float64,1}}"><code>Games.worst_value_1</code></a> — <span class="docstring-category">Method</span>.</div><div><p>See worst_value_i for documentation</p></div><a class="source-link" target="_blank" href="https://github.com/QuantEcon/Games.jl/tree/cafd8e9206d4126611afce9ffcbdc472b5e06bcc/src/repeated_game.jl#L196">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Games.worst_value_2-Tuple{Games.RepeatedGame{2,T} where T&lt;:Real,Array{Float64,2},Array{Float64,1}}" href="#Games.worst_value_2-Tuple{Games.RepeatedGame{2,T} where T&lt;:Real,Array{Float64,2},Array{Float64,1}}"><code>Games.worst_value_2</code></a> — <span class="docstring-category">Method</span>.</div><div><p>See worst_value_i for documentation</p></div><a class="source-link" target="_blank" href="https://github.com/QuantEcon/Games.jl/tree/cafd8e9206d4126611afce9ffcbdc472b5e06bcc/src/repeated_game.jl#L199">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Games.worst_value_i-Tuple{Games.RepeatedGame{2,T} where T&lt;:Real,Array{Float64,2},Array{Float64,1},Int64}" href="#Games.worst_value_i-Tuple{Games.RepeatedGame{2,T} where T&lt;:Real,Array{Float64,2},Array{Float64,1},Int64}"><code>Games.worst_value_i</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Given a constraint w ∈ W, this finds the worst possible payoff for agent i</p><p>The output of this function is used to create the values associated with incentive compatibility constraints</p><p>The arguments for this function are</p><ul><li><p>rpd: Two player repeated game</p></li><li><p>H: Subgradients used to approximate value set</p></li><li><p>C: Hyperplane levels for value set approximation</p></li><li><p>i: Which player want worst value for</p></li></ul></div><a class="source-link" target="_blank" href="https://github.com/QuantEcon/Games.jl/tree/cafd8e9206d4126611afce9ffcbdc472b5e06bcc/src/repeated_game.jl#L165-L176">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Games.worst_values-Tuple{Games.RepeatedGame{2,T} where T&lt;:Real,Array{Float64,2},Array{Float64,1}}" href="#Games.worst_values-Tuple{Games.RepeatedGame{2,T} where T&lt;:Real,Array{Float64,2},Array{Float64,1}}"><code>Games.worst_values</code></a> — <span class="docstring-category">Method</span>.</div><div><p>See worst_value_i for documentation</p></div><a class="source-link" target="_blank" href="https://github.com/QuantEcon/Games.jl/tree/cafd8e9206d4126611afce9ffcbdc472b5e06bcc/src/repeated_game.jl#L202">source</a><br/></section><h3><a class="nav-anchor" id="Internal-2" href="#Internal-2">Internal</a></h3><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Games.initialize_LP_matrices-Tuple{Games.RepeatedGame{2,T} where T&lt;:Real,Any}" href="#Games.initialize_LP_matrices-Tuple{Games.RepeatedGame{2,T} where T&lt;:Real,Any}"><code>Games.initialize_LP_matrices</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Initialize matrices for the linear programming problems. It sets up the A matrix since it never changes, but only allocates space for b and c since they will be filled repeatedly on the iterations</p><p>We add nH slack variables (which will be constrained to be positive) to deal with inequalities associated with Ax leq b.</p><p>min c ⋅ x     Ax &lt; b</p><p>In this case, the <code>c</code> vector will be determined by which subgradient is being used, so this function only allocates space for it.</p><p>The <code>A</code> matrix will be filled with nH set constraints and 2 incentive compatibility constraints. The set constraints restrain the linear programming problem to pick solutions that are in the current set of continuation values while the incentive compatibility constraints ensure the agents won&#39;t deviate.</p><p>The <code>b</code> vector is associated with the <code>A</code> matrix and gives the value for constraint.</p><p>The arguments for this function are</p><ul><li><p>rpd: Two player repeated game</p></li><li><p>H: The subgradients used to approximate the value set</p></li></ul></div><a class="source-link" target="_blank" href="https://github.com/QuantEcon/Games.jl/tree/cafd8e9206d4126611afce9ffcbdc472b5e06bcc/src/repeated_game.jl#L119-L143">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Games.initialize_sg_hpl-Tuple{Games.RepeatedGame,Int64}" href="#Games.initialize_sg_hpl-Tuple{Games.RepeatedGame,Int64}"><code>Games.initialize_sg_hpl</code></a> — <span class="docstring-category">Method</span>.</div><div><p>This is a function that initializes the subgradients, hyperplane levels, and extreme points of the value set by choosing an appropriate origin and radius.</p><p>See <code>initialize_sg_hpl</code> for more documentation</p></div><a class="source-link" target="_blank" href="https://github.com/QuantEcon/Games.jl/tree/cafd8e9206d4126611afce9ffcbdc472b5e06bcc/src/repeated_game.jl#L95-L101">source</a><br/></section><section class="docstring"><div class="docstring-header"><a class="docstring-binding" id="Games.initialize_sg_hpl-Tuple{Int64,Array{Float64,1},Float64}" href="#Games.initialize_sg_hpl-Tuple{Int64,Array{Float64,1},Float64}"><code>Games.initialize_sg_hpl</code></a> — <span class="docstring-category">Method</span>.</div><div><p>Initializes the following things for a 2 player repeated game.</p><ul><li><p>subgradients</p></li><li><p>extreme points of the convex set for values</p></li><li><p>hyper plane levels</p></li></ul><p>These are determined in the following way</p><ul><li><p>Subgradients are simply chosen from the unit circle.</p></li><li><p>The values for the extremum of the value set are just given by choosing points along a circle with specified origin and radius</p></li><li><p>Hyperplane levels are determined by computing the hyperplane level such that the extreme points from the circle are generated</p></li></ul><p>The arguments are</p></div><a class="source-link" target="_blank" href="https://github.com/QuantEcon/Games.jl/tree/cafd8e9206d4126611afce9ffcbdc472b5e06bcc/src/repeated_game.jl#L59-L74">source</a><br/></section><footer><hr/><a class="previous" href="computing_nash_equilibria.html"><span class="direction">Previous</span><span class="title">Computing Nash Equilibria</span></a><a class="next" href="random.html"><span class="direction">Next</span><span class="title">Random</span></a></footer></article></body></html>
